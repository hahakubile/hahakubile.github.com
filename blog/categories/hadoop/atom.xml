<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | hahakubile Blog]]></title>
  <link href="http://hahakubile.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://hahakubile.github.io/"/>
  <updated>2014-02-20T03:30:02-05:00</updated>
  <id>http://hahakubile.github.io/</id>
  <author>
    <name><![CDATA[hahakubile]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Compile Sqoop With Specified Hadoop Version]]></title>
    <link href="http://hahakubile.github.io/blog/2014/02/17/how-to-compile-sqoop-with-specified-hadoop-version/"/>
    <updated>2014-02-17T01:11:17-05:00</updated>
    <id>http://hahakubile.github.io/blog/2014/02/17/how-to-compile-sqoop-with-specified-hadoop-version</id>
    <content type="html"><![CDATA[<h2>command</h2>

<ul>
<li>To compile Sqoop, run <code>ant package</code>. There will be a fully self-hosted build provided in the <code>build/sqoop-(version)/</code> directory.</li>
<li>You can build just the jar by running <code>ant jar</code>.</li>
<li>You can specify the <code>hadoopversion</code>, <code>ant -Dhadoopversion=20 package</code></li>
</ul>


<h2>hadoopversion</h2>

<p><a href="https://github.com/apache/sqoop/blob/trunk/build.xml">hadoopversion</a>. Can only be 20, 23, 100, 200 or 210</p>

<h3>20</h3>

<pre><code>
arg1="${hadoopversion}" arg2="20"
name="hadoop.version" value="0.20.2-cdh3u1"
name="hbase94.version" value="0.90.3-cdh3u1"
name="zookeeper.version" value="3.3.3-cdh3u1"
</code></pre>


<h3>23</h3>

<pre><code>
arg1="${hadoopversion}" arg2="23"
name="hadoop.version" value="0.23.1"
name="hbase94.version" value="0.92.0"
name="zookeeper.version" value="3.4.2"
</code></pre>


<h3>100</h3>

<pre><code>
arg1="${hadoopversion}" arg2="100"
name="hadoop.version" value="1.0.0"
name="hbase94.version" value="0.92.0"
name="zookeeper.version" value="3.4.2"
</code></pre>


<h3>200</h3>

<pre><code>
name="hadoop.version" value="2.0.4-alpha"
name="hbase94.version" value="0.94.2"
name="zookeeper.version" value="3.4.2"
</code></pre>


<h3>210</h3>

<pre><code>
name="hadoop.version" value="2.1.0-beta"
name="hbase94.version" value="0.94.2"
name="zookeeper.version" value="3.4.2"
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Configuring Ulimit for HBase in Cloudera Cdh3u6]]></title>
    <link href="http://hahakubile.github.io/blog/2014/02/13/configuring-ulimit-for-hbase-in-cloudera-cdh3u6/"/>
    <updated>2014-02-13T02:57:04-05:00</updated>
    <id>http://hahakubile.github.io/blog/2014/02/13/configuring-ulimit-for-hbase-in-cloudera-cdh3u6</id>
    <content type="html"><![CDATA[<h2>Error in hbase regionserver log</h2>

<pre><code>2014-02-13 11:38:13,028 INFO org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.101.21:50010, add to deadNodes and continue
java.net.SocketException: Too many open files
</code></pre>

<h2>About ulimit</h2>

<p><a href="http://hbase.apache.org/book.html#ulimit">http://hbase.apache.org/book.html#ulimit</a></p>

<p>To be clear, upping the file descriptors and nproc for the user who is running the HBase process is an operating system configuration, not an HBase configuration.</p>

<h2><a href="http://essayboard.com/2011/05/28/raising-ulimit-on-centos-server/">Raising ulimit on centos-server</a></h2>

<h3>fs.file-max</h3>

<ol>
<li><code>vim /etc/sysctl.conf</code></li>
<li>Add this line <code>fs.file-max = 122880</code> into <code>/etc/sysctl.conf</code></li>
<li>You can execute the command <code>sysctl -p</code> or reboot to permanently add the kernel parameter above

<h3>ulimit</h3></li>
<li><code>vim /etc/security/limits.conf</code></li>
<li>Add the lines below:

<ul>
<li>soft nofile 122880</li>
<li>hard nofile 122880</li>
</ul>
</li>
<li>logout and login, execute the command <code>ulimit -n</code> inside your terminal.
If you had done everything correctly, you should now see a value returns as 122880.</li>
</ol>


<h2>CM users</h2>

<p>For CM users, since there&rsquo;s a supervisor in effect that monitors the launched daemons, the ulimits are inherited from it and are hence pre-set in <code>/usr/sbin/cmf-agent</code> (look for lines that start with &ldquo;ulimit&rdquo;.</p>

<ol>
<li>Edit /usr/sbin/cmf-agent and change the ulimit -n setting.</li>
<li>shutdown hdfs/mapreduce/hbase services on them.</li>
<li>On these stopped nodes run: <code>service cloudera-scm-agent hard_restart</code></li>
<li>Restart the services.</li>
</ol>


<h2>How to confirm</h2>

<p>To confirm, you can go to any CM-managed node, <code>ps -ef | grep hdfs</code>, grab the pid of a hdfs process.
<code>cat /proc/&lt;pid&gt;/fd</code> to make sure it&rsquo;s sticking.</p>

<pre><code>Limit                     Soft Limit           Hard Limit           Units     
Max processes             65536                65536                processes 
Max open files            122880               122880               files 
</code></pre>
]]></content>
  </entry>
  
</feed>
