<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hbase | hahakubile Blog]]></title>
  <link href="http://hahakubile.github.io/blog/categories/hbase/atom.xml" rel="self"/>
  <link href="http://hahakubile.github.io/"/>
  <updated>2014-02-24T01:12:40-05:00</updated>
  <id>http://hahakubile.github.io/</id>
  <author>
    <name><![CDATA[hahakubile]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HBase Region Pre-Splitting]]></title>
    <link href="http://hahakubile.github.io/blog/2014/02/20/hbase-region-pre-splitting/"/>
    <updated>2014-02-20T02:52:37-05:00</updated>
    <id>http://hahakubile.github.io/blog/2014/02/20/hbase-region-pre-splitting</id>
    <content type="html"><![CDATA[<h3>Background</h3>

<p>Hbase Version: 0.90.6</p>

<h3>RegionSplitter</h3>

<h4>Usage</h4>

<p>The <code>org.apache.hadoop.hbase.util.RegionSplitter</code> class provides several utilities to create a table with a specified number of pre-split regions.</p>

<p>create a table named &lsquo;myTable&rsquo; with 60 pre-split regions containing 2 column families &lsquo;test&rsquo; &amp; &lsquo;rs&rsquo;</p>

<p><code>bin/hbase org.apache.hadoop.hbase.util.RegionSplitter -c 60 -f test:rs myTable</code></p>

<ul>
<li>-c 60, specifies the requested number of regions as 60</li>
<li>-f specifies the column families you want in the table, separated by &ldquo;:&rdquo;</li>
</ul>


<h4>SplitAlgorithm</h4>

<p><code>MD5StringSplit</code> is the default RegionSplitter.SplitAlgorithm for creating pre-split tables. The format of MD5StringSplit is the ASCII representation of an MD5 checksum. Row are long values in the range &ldquo;00000000&rdquo; => &ldquo;7FFFFFFF&rdquo; and are left-padded with zeros to keep the same order lexographically as if they were binary.</p>

<pre><code>
public static class MD5StringSplit implements SplitAlgorithm {
    final static String MAXMD5 = "7FFFFFFF";
    final static BigInteger MAXMD5_INT = new BigInteger(MAXMD5, 16);

    public byte[][] split(int n) {
        BigInteger[] splits = new BigInteger[n - 1];
        BigInteger sizeOfEachSplit = MAXMD5_INT.divide(BigInteger.valueOf(n));
        for (int i = 1; i < n; i++) {
            // NOTE: this means the last region gets all the slop.
            // This is not a big deal if we're assuming n << MAXMD5
            splits[i - 1] = sizeOfEachSplit.multiply(BigInteger.valueOf(i));
        }
        return convertToBytes(splits);
    }

    ...

}
</code></pre>


<h4>Rowkey Design</h4>

<pre><code>
    String id = "your_id";

    String md5 = org.apache.commons.codec.digest.DigestUtils.md5Hex(id);
    BigInteger x1 = new BigInteger(md5, 16);
    BigInteger x2 = new BigInteger("7FFFFFFF", 16);
    BigInteger x3 = x1.and(x2);
                      
    String rowkey_hash = x3.toString(16);
    while(rowkey_hash.length() < 8){
        rowkey_hash = "0" + rowkey_hash;
    }

    String rowkey = rowkey_hash + "_" + id;
    System.out.println(rowkey);
</code></pre>


<h3>Scenario</h3>

<ul>
<li>Data Size: 4TB</li>
<li>HBase Cluster: 10 nodes</li>
<li>hbase.hregion.max.filesize: 2GB</li>
<li>Cluster Region Count: 4TB/2GB = 2048</li>
<li>Node Region Count: 2048/10 ~= 200</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bulk Load Huge Data From Mysql to HBase Using Sqoop]]></title>
    <link href="http://hahakubile.github.io/blog/2014/02/13/bulk-load-huge-data-from-mysql-to-hbase-using-sqoop/"/>
    <updated>2014-02-13T21:44:36-05:00</updated>
    <id>http://hahakubile.github.io/blog/2014/02/13/bulk-load-huge-data-from-mysql-to-hbase-using-sqoop</id>
    <content type="html"><![CDATA[<h2>MySQL->HBase:HBaseImportJob</h2>

<h2>MySQL->HBase:HBaseBulkImportJob</h2>

<h2>MySQL->HDFS(Avro files)&ndash;>HBase</h2>

<p>Sqoop from MySQL into Avro files in HDFS, this will result in a bunch of avro files in your target directory. You can check that
they&rsquo;re there with <code>hadoop fs -ls $TARGET_DIR</code>.</p>

<p><code>sqoop import --connect jdbc:mysql://$MYSQL_SERVER/$DATABASE --driver com.mysql.jdbc.Driver --username $USER --password $PASSWORD --table $TABLE --target-dir $TARGET_DIR --as-avrodatafile</code></p>
]]></content>
  </entry>
  
</feed>
